{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1e05fa-d745-4dc9-8285-a4a260a87d33",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99893bb4-1f8f-4a0f-9238-98d4cba46bb3",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is an essential and modern branch of machine learning, where an agent learns through trial and error. RL involves an agent that observes and acts within an environment receiving rewards for good decisions and penalties for bad ones. The agentâ€™s goal is to devise a strategy that maximizes positive feedback over time.\n",
    "\n",
    "* Agent learns through trial and error\n",
    "* Agent receives:\n",
    "    * Rewards for good decisions\n",
    "    * Penalties for bad decisions\n",
    "* Goal: maximize positive feedback over time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fccc066-fa32-4383-a625-b6043511e645",
   "metadata": {},
   "source": [
    "|                        | **Supervised Learning**         | **Unsupervised Learning**                   | **Reinforcement Learning (RL)**                          |\n",
    "|------------------------|----------------------------------|---------------------------------------------|----------------------------------------------------------|\n",
    "| **Data Type**          | Labeled data                    | Unlabeled data                              | No predefined training data                              |\n",
    "| **Main Objective**     | Predict outcomes based on input data | Discover underlying patterns or associations in data | Make decisions that maximize reward from the environment |\n",
    "| **Suitability**        | Classification, regression      | Clustering, association analysis            | Decision-making tasks                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a225b84-f6da-4e35-b318-29fba619668e",
   "metadata": {},
   "source": [
    "**Supervised learning**\n",
    "\n",
    "- Learns to predict outcomes based on labeled data\n",
    "- Used to perform classification tasks\n",
    "\n",
    "**Unsupervised learning**\n",
    "\n",
    "- Used to perform clustering\n",
    "- Finds patterns and relationships in unlabeled data\n",
    "\n",
    "**Reinforcement learning**\n",
    "- Uses reward and punishment to guide learning\n",
    "- Learns to make decisions through trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93fcb3-a188-469d-922b-6ca2c0d921ee",
   "metadata": {},
   "source": [
    "RL is well-suited for scenarios that require training a model to make sequential decisions where each decision influences future observations. In this setting, the agent learns through rewards and penalties. These guide it towards developing more effective strategies without any kind of direct supervision.\n",
    "\n",
    "* Sequential decision-making\n",
    "    * Decisions influence future observations\n",
    "    * Learning through rewards and penalties\n",
    "* No direct supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31649002-5783-4ab1-b807-0b3ed8d4dbdd",
   "metadata": {},
   "source": [
    "## RL framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deaea1b-3034-421c-9486-fdf535be537e",
   "metadata": {},
   "source": [
    "The RL framework consists of five key components: the agent, environment, states, actions, and rewards. The agent, acting as the learner or decision-maker, is like a player in a game. It interacts with the environment, which presents various challenges to be solved. Within this environment, a state represents a specific moment in time, much like video game frame, capturing the current situation that the agent observes. The agent's actions are responses to these states, and rewards from the environment are feedback on these actions, either positive to encourage or negative to discourage certain behaviors.\n",
    "\n",
    "* **Agent**: learner, decision-maker\n",
    "* **Environment**: challenges to be solved\n",
    "* **State**: environment snapshot at given time\n",
    "* **Action**: agent's choice in response to state\n",
    "* **Reward**: feedback for agent action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd569f04-6de3-474f-b8bc-4456fb4e18dc",
   "metadata": {},
   "source": [
    "## Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136ab7b-bc09-43e2-96fc-2a433f283330",
   "metadata": {},
   "source": [
    "Markov Decision Processes (MDP) provides a mathematical framework for modeling RL environments. It simplifies complex environments by defining four key components: states, actions, rewards, and transition probabilities which represent the likelihood of moving from one state to another following an action.\n",
    "\n",
    "At the heart of MDPs is the Markov property, which states that the future state depends only on the current state and action, not on previous events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eacc5fb-429f-4b22-9c8b-d47380a45e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597efbcb-044a-48aa-a7c2-9fc022c34f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f51302-1837-460d-88d1-876c5e89b448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3929d-c0d1-4f5b-ad31-a0316df4be50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2b10-c8b0-41fe-b076-15ac02e1a366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4b81a-7bd6-418c-8d72-c760e7fb2759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31ee28-25fc-4156-92bf-718f1c92cdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8b750-84eb-4b5f-a639-6ec5c8d6f838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3659b0e-b7ac-4b6b-910f-b9aaa70e73b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010a335-9aea-4432-af15-0532e8906173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
