{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843bb183-871b-463c-b219-c0e72b81c3d5",
   "metadata": {},
   "source": [
    "# Problém vícerukého bandity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72dd30-ece2-42f5-a8bf-5d93a75a7524",
   "metadata": {},
   "source": [
    "Zdroj: https://www.geeksforgeeks.org/machine-learning/multi-armed-bandit-problem-in-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db274cbc-5aaa-415f-b541-f22572f0b3cd",
   "metadata": {},
   "source": [
    "Problém mnohorukého bandity (Multi-Armed Bandit - MAB) je klasický problém v teorii pravděpodobnosti, který vystihuje podstatu vyvažování mezi průzkumem (**exploration**) a využitím (**exploitation**). Tento problém je pojmenován podle scénáře, kdy hráč čelí více výherním automatům (banditům) a potřebuje určit, na kterém automatu hrát, aby maximalizoval svou odměnu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c38d5-f04d-457b-9c99-2ecd4d13a9cf",
   "metadata": {},
   "source": [
    "## Pochopení problému vícerukého bandity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b8aea-4da4-44e3-92c6-71dfaf534b20",
   "metadata": {},
   "source": [
    "### Definice problému"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92374062-04d4-417b-bd2c-3fb35226b972",
   "metadata": {},
   "source": [
    "V problému vícerukého bandity je agentovi nabídnuto několik možností (rukou - ramen), z nichž každá poskytuje odměnu vyvozenou z neznámého rozdělení pravděpodobnosti. Agent se snaží maximalizovat kumulativní odměnu v průběhu série pokusů. Úkol spočívá ve výběru nejlepšího ramene k provedení testu, ve vyvážení potřeby prozkoumat různá ramena a zjistit jejich rozdělení odměn a využít známá ramena, která poskytla vysoké odměny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a8ff8-c929-4e2a-9c26-46ce21c95dc6",
   "metadata": {},
   "source": [
    "### Formální zastoupení"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40e1f6-20dd-4665-9ac9-e367c4169185",
   "metadata": {},
   "source": [
    "Formálně lze problém MAB popsat následovně:\n",
    "\n",
    "* **Ramena**: $K$ nezávislých ramen, každé s neznámým rozdělením odměn.\n",
    "* **Odměny**: Každá $i$-tá větev poskytuje odměnu $R_i$, která je dána neznámým rozdělením s očekávanou hodnotou $μ_i$.\n",
    "* **Cíl**: Maximalizovat kumulativní odměnu v průběhu $T$ pokusů."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616c79e-f920-45ab-b687-07310d2d739a",
   "metadata": {},
   "source": [
    "Ústředním dilematem problému MAB je kompromis mezi průzkumem (**exploration**) (vyzkoušením různých odvětví za účelem získání informací o jejich odměnách) a využitím (**exploitation**) (výběrem odvětví, které na základě aktuálních informací poskytlo nejvyšší odměny). Vyvážení těchto dvou aspektů je klíčové pro optimalizaci dlouhodobých odměn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1404d57-46c4-4044-bfd7-7512b058c9c5",
   "metadata": {},
   "source": [
    "## Strategie pro řešení problému vícerukého bandity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75421a2b-c535-445d-93d3-a1eab29a490f",
   "metadata": {},
   "source": [
    "Pro řešení problému MAB bylo vyvinuto několik strategií. Zde je několik z nejvýznamnějších algoritmů:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8656c-b8e0-4d56-85cd-a3e686081c29",
   "metadata": {},
   "source": [
    "### 1. Epsilon - Chamtivý (Epsilon-Greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501e75d-88e9-4dc7-85bc-097e0b3a13a9",
   "metadata": {},
   "source": [
    "Epsilonový algoritmus je jednou z nejjednodušších strategií pro řešení problému MAB. Funguje následovně:\n",
    "\n",
    "* S pravděpodobností $ϵ$ prozkoumejte náhodné rameno.\n",
    "* S pravděpodobností $1−ϵ$ prozkoumejte rameno s nejvyšší odhadovanou odměnou.\n",
    "\n",
    "**Algoritmus Epsilon-Greedy**\n",
    "\n",
    "1. Inicializujte odhadované hodnoty všech ramen na nulu nebo malé kladné číslo.\n",
    "2. Pro každý pokus:\n",
    "    - Vygenerujte náhodné číslo mezi 0 a 1.\n",
    "    - Pokud je číslo menší než $ϵ$, vyberte náhodnou skupinu (**exploration**).\n",
    "    - V opačném případě vyberte skupinu s nejvyšší odhadovanou odměnou (**exploitation**).\n",
    "    - Aktualizujte odhadovanou odměnu vybrané skupiny na základě pozorované odměny.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e230ce2-a4fd-4c00-aa93-2c751831dabc",
   "metadata": {},
   "source": [
    "**Implementace v Pythonu**\n",
    "\n",
    "Implementace demonstruje algoritmus Epsilon-Greedy, což je běžná strategie pro řešení problému Multi-Armed Bandit (MAB). Kód si klade za cíl ilustrovat, jak může agent vyvážit průzkum (**exploration**) a VYUŽITÍ (**exploitation**), aby maximalizoval svou kumulativní odměnu.\n",
    "\n",
    "1. **Simulace problému vícerukého bandity**: Kód simuluje scénář, ve kterém se agent musí rozhodnout, který z několika výherních automatů (ramen) vytáhne, aby maximalizoval celkovou přijatou odměnu.\n",
    "2. **Implementujte algoritmus Epsilon-Greedy**: Epsilon-Greedy je jednoduchý, ale efektivní algoritmus, který vyvažuje potřebu prozkoumat nové možnosti (ramena) a využít známé a výnosné možnosti.\n",
    "3. **Vyhodnocení výkonu**: Implementace sleduje celkovou odměnu nashromážděnou během série pokusů, aby vyhodnotila účinnost strategie Epsilon-Greedy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8927b34e-8f3a-41e9-a290-74979005eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, n_arms, epsilon):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = np.zeros(n_arms)  # Number of times each arm is pulled\n",
    "        self.values = np.zeros(n_arms)  # Estimated values of each arm\n",
    "\n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        else:\n",
    "            return np.argmax(self.values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612f2cf-2696-4c02-9643-cbfc9d470773",
   "metadata": {},
   "source": [
    "Importujeme knihovnu `numpy`, která se používá pro práci s vektory, maticemi a náhodnými čísly.\n",
    "\n",
    "`class EpsilonGreedy:` Definuje agenta, který bude rozhodovat, kterou páku (rameno) tahat.\n",
    "\n",
    "`__init__` definuje `n_arms` počet pák, `epsilon` pravděpodobnost, že bude agent prozkoumávat, `counts` kolikrát byla každá páka vybrána (pole nul), `values` odhadovaná průměrná odměna každé páky (pole nul).\n",
    "\n",
    "`select_arm` definuje jak agent vybírá páku. Vygeneruje náhodné číslo mezi 0 a 1. Pokud je menší než $epsilon$, agent exploruje → vybere náhodnou páku. Jinak exploatuje → vybere tu, která má aktuálně nejvyšší odhadovanou hodnotu (argmax).\n",
    "Pokud epsilon = 0.1, pak v 10 % případů náhodně zkouší, jinak využívá nejlepší známou volbu.\n",
    "\n",
    "`update` definuje aktualizaci hodnot. `counts` zvýšíme počet tahů pro danou páku. `values[chosen_arm]` spočítáme nový odhad průměrné odměny pro tuto páku. Je to způsob, jak postupně zpřesňovat odhad, aniž by bylo potřeba ukládat všechny odměny z minulosti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d182925-236a-4d32-98bd-c142b2f30f9a",
   "metadata": {},
   "source": [
    "* **Příklad použití**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f967e0e2-eb48-48b0-88f6-851686b53714",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "epsilon = 0.1\n",
    "n_trials = 1000\n",
    "rewards = np.random.randn(n_arms, n_trials)\n",
    "agent = EpsilonGreedy(n_arms, epsilon)\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d9d5ce-5b73-4953-b041-f0c8f097438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 7.768582192994807\n"
     ]
    }
   ],
   "source": [
    "for t in range(n_trials):\n",
    "    arm = agent.select_arm()\n",
    "    reward = rewards[arm, t]\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70dee0-0589-41ad-8c9b-80eae21004b4",
   "metadata": {},
   "source": [
    "`for t in range(n_trials):` Opakuje se `n_trials` (1000) pokusů. Každý pokus simuluje jedno rozhodnutí agenta – tedy výběr jedné páky a obdržení odměny. Proměnná `t` označuje aktuální tah, ale v tomto konkrétním kódu se nepoužívá jako index v čase, ale jako sloupec ve `rewards`.\n",
    "\n",
    "`arm = agent.select_arm()` Agent se rozhodne, kterou páku si zvolí. S pravděpodobností $epsilon$ si vybere náhodně (**exploration**).\n",
    "Jinak si zvolí tu, která má zatím nejvyšší očekávanou hodnotu (**exploitation**).\n",
    "\n",
    "`reward = rewards[arm, t]` Agent získá simulovanou odměnu z matice `rewards` za tahání za páku arm ve `t`-tém pokusu.\n",
    "\n",
    "`agent.update(arm, reward)` Agent si aktualizuje své znalosti. Zvýší počet tahů pro tuto páku. Přepočítá odhadovanou průměrnou odměnu této páky.\n",
    "\n",
    "`total_reward += reward` Přičte odměnu z tohoto tahu do celkové odměny. Funguje jako metrika úspěšnosti strategie. Čím víc agent tahá za správné páky, tím vyšší `total_reward`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea31b0e-607a-45f6-a5b2-6c8c730bcc20",
   "metadata": {},
   "source": [
    "### 2. Horní hranice spolehlivosti (Upper Confidence Bound - UCB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a67efba-e8a9-4c8c-94b9-61f7ac1f032a",
   "metadata": {},
   "source": [
    "Algoritmus UCB je založen na principu optimismu tváří v tvář nejistotě. Vybírá skupinu s nejvyšší horní hranicí spolehlivosti a vyvažuje odhadovanou odměnu a nejistotu odhadu.\n",
    "\n",
    "**Algoritmus**\n",
    "1. Inicializujte počty a hodnoty všech ramen.\n",
    "2. Pro každý pokus:\n",
    "    - Vypočítejte horní hranici spolehlivosti pro každé rameno $UCB_i = \\hat{\\mu}_i + \\sqrt{\\frac{2 \\ln t}{N}}$ , kde $\\hat{\\mu}_i$ je odhadovaná odměna, $t$ je aktuální pokus a $N$ je počet pokusů o provedení $i$-té větvičky.\n",
    "    - Vyberte rameno s nejvyšší hodnotou UCB.\n",
    "    - Aktualizujte odhadovanou odměnu vybraného ramena na základě pozorované odměny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae07ca-1e17-46ae-8c5f-872e60e8bca6",
   "metadata": {},
   "source": [
    "**Implementace v Pythonu**\n",
    "\n",
    "Implementace demonstruje algoritmus horní hranice spolehlivosti (UCB). Vysvětlení cílů a kroků této implementace.\n",
    "\n",
    "1. **Simulace problému vícerukého bandity**: Kód simuluje scénář, kdy agent čelí více výherním automatům (rameným) a musí se rozhodnout, které rameno vybere, aby maximalizoval odměny.\n",
    "2. **Aplikace algoritmu horní hranice spolehlivosti (UCB)**: Algoritmus UCB vybírá ramena na základě jejich odhadovaných odměn a nejistoty těchto odhadů s cílem efektivně vyvážit průzkum a využití.\n",
    "3. **Vyhodnocení výkonu**: Implementace sleduje celkovou odměnu nashromážděnou v průběhu série pokusů, aby vyhodnotila, jak dobře algoritmus UCB dosahuje maximalizace odměny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31f2d7fc-9851-4456-9732-0acb22cdc7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class UCB:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self):\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(self.total_counts + 1) / (self.counts + 1e-5))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1\n",
    "        self.total_counts += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df29b0d-e549-42f8-befd-6c541d301be4",
   "metadata": {},
   "source": [
    "Importujeme knihovnu `numpy`, která se používá pro práci s vektory, maticemi a náhodnými čísly.\n",
    "\n",
    "`class UCP`: Definuje agenta, který bude rozhodovat, kterou páku (rameno) tahat.\n",
    "\n",
    "`__init__` definuje `n_arms` počet pák, `counts` kolikrát byla každá páka vybrána (pole nul), `values` odhadovaná průměrná odměna každé páky (pole nul), `total_counts` celkový počet tahů napříč všemi pákami.\n",
    "\n",
    "`select_arm` definuje jak agent vybírá páku. **exploitation** = `self.values` (dosavadní průměrné odměny) + **exploration** = `np.sqrt(2 * np.log(self.total_counts + 1) / (self.counts + 1e-5))`, kde `1e-5` je malá konstanta pro zabránění dělení nulou. \n",
    "$$\n",
    "\\sqrt{ \\frac{2 \\cdot \\log(\\text{celkový počet pokusů + 1})}{\\text{počet pokusů dané páky + 1e-5}} }\n",
    "$$\n",
    "`np.argmax(ucb_values)` vrací index páky s nejvyšší \"upper confidence bound\" – tedy páky, která má buď vysokou odměnu, nebo ještě není dostatečně prozkoumaná.\n",
    "\n",
    "`update` definuje aktualizaci hodnot. `counts` zvýšíme počet tahů pro danou páku. `values[chosen_arm]` spočítáme nový odhad průměrné odměny pro tuto páku. Je to způsob, jak postupně zpřesňovat odhad, aniž by bylo potřeba ukládat všechny odměny z minulosti."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d59530-a1dc-4db2-80dd-edc583490cc3",
   "metadata": {},
   "source": [
    "* **Příklad použití**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9422dda5-bcfd-4675-a98b-eeece60cdc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "n_trials = 1000\n",
    "rewards = np.random.randn(n_arms, n_trials)\n",
    "agent = UCB(n_arms)\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef86265-679e-4358-ace7-3511f04a26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 15.897621183229866\n"
     ]
    }
   ],
   "source": [
    "for t in range(n_trials):\n",
    "    arm = agent.select_arm()\n",
    "    reward = rewards[arm, t]\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d439b-f387-4c1a-aa60-6987a03d3d64",
   "metadata": {},
   "source": [
    "`for t in range(n_trials):`\n",
    "Opakuje se `n_trials` (např. 1000) pokusů. Každý pokus simuluje jedno rozhodnutí agenta – tedy výběr jedné páky a obdržení odměny. Proměnná `t` označuje aktuální tah, v tomto kódu se používá jako sloupec v matici `rewards`.\n",
    "\n",
    "`arm = agent.select_arm()` Agent zvolí páku podle algoritmu UCB. \n",
    "\n",
    "`reward = rewards[arm, t]` Agent získá simulovanou odměnu z matice rewards za tahání za páku ve `t`-tém pokusu.\n",
    "\n",
    "`agent.update(arm, reward)` Agent si aktualizuje své znalosti – zvýší počet výběrů této páky a přepočítá její průměrnou odměnu.\n",
    "\n",
    "`total_reward += reward` Přičte odměnu z tohoto tahu do celkové odměny. Čím častěji agent volí výhodné páky, tím vyšší bude total_reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311247e-f7ec-4e16-a04b-2fe032749a50",
   "metadata": {},
   "source": [
    "### Thompsonovo vzorkování (Thompson Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1aa37-cd2c-4045-8eb6-77817a0aec49",
   "metadata": {},
   "source": [
    "Thompsonův přístup je Bayesovský přístup k problému MAB. Zachovává rozdělení pravděpodobnosti pro odměnu každé skupiny a vybírá skupiny na základě výběrů z těchto rozdělení."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd17f6a-bb62-4956-8663-59be024ca807",
   "metadata": {},
   "source": [
    "**Algoritmus**\n",
    "\n",
    "1. Inicializujte parametry rozdělení odměn (např. beta rozdělení) pro každou skupinu.\n",
    "2. Pro každý pokus:\n",
    "     - Vyberte odhad odměny z rozdělení každé skupiny.\n",
    "     - Vyberte skupinu s nejvyšší vzorkovanou odměnou.\n",
    "     - Aktualizujte parametry rozdělení vybrané skupiny na základě pozorované odměny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b7fb4-60da-40bf-8829-7071bbfa522c",
   "metadata": {},
   "source": [
    "**Implementace v Pythonu**\n",
    "\n",
    "Implementace si klade za cíl demonstrovat algoritmus Thompson Sampling, Bayesovský přístup k řešení problému Multi-Armed Bandit (MAB).\n",
    "\n",
    "* **Simulace problému vícerukého bandity**: Kód simuluje scénář, kdy agent čelí více výherním automatům (rameným) a musí se rozhodnout, které rameno vybere, aby maximalizoval odměny.\n",
    "* **Aplikace algoritmu Thompson Sampling**: Thompson Sampling je pravděpodobnostní algoritmus, který vyvažuje průzkum a exploataci vzorkováním z posteriorních rozdělení odměny každého ramene.\n",
    "* **Vyhodnocení výkonu**: Implementace sleduje celkovou odměnu nashromážděnou v průběhu série pokusů, aby vyhodnotila, jak dobře algoritmus Thompson Sampling funguje při maximalizaci odměny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7b8110-9c82-417a-90f9-8fd45352f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.successes = np.zeros(n_arms)\n",
    "        self.failures = np.zeros(n_arms)\n",
    "\n",
    "    def select_arm(self):\n",
    "        sampled_values = np.random.beta(self.successes + 1, self.failures + 1)\n",
    "        return np.argmax(sampled_values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        if reward > 0:\n",
    "            self.successes[chosen_arm] += 1\n",
    "        else:\n",
    "            self.failures[chosen_arm] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a2c05-6c48-49a8-ba13-e496eb909a7f",
   "metadata": {},
   "source": [
    "Importujeme knihovnu `numpy`, která se používá pro práci s vektory, maticemi a náhodnými čísly.\n",
    "\n",
    "`class ThompsonSampling`: Definuje agenta, který bude rozhodovat, kterou páku (rameno) tahat.\n",
    "\n",
    "`__init__` definuje `n_arms` počet pák, `successes` počet úspěšných odměn pro každou páku (pole nul), `failures` počet neúspěchů (pole nul).\n",
    "\n",
    "`select_arm` definuje jak agent vybírá páku. `np.random.beta(self.successes + 1, self.failures + 1)` Pro každou páku si agent vytáhne vzorek z Beta rozdělení $Beta(úspěchy +1, neúspěchy +1)$. Tímto způsobem je pravděpodobnější volba pák s vyšším očekáváním, ale občas se zkouší i ty nejisté.\n",
    "\n",
    "`update` definuje aktualizaci hodnot. Pokud `reward` > 0, považuje se to za úspěch → zvýší se počet úspěchů. Jinak jde o neúspěch → zvýší se počet neúspěchů.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86bd831-612d-47e5-875a-0642dbe930b0",
   "metadata": {},
   "source": [
    "* **Příklad použití**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00429d27-5b8e-4c78-8620-052a06e30917",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "n_trials = 1000\n",
    "rewards = np.random.randn(n_arms, n_trials)\n",
    "agent = ThompsonSampling(n_arms)\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ea327aa-b319-4ade-a3d4-87e0e46ffa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 7.957880789747213\n"
     ]
    }
   ],
   "source": [
    "for t in range(n_trials):\n",
    "    arm = agent.select_arm()\n",
    "    reward = rewards[arm, t]\n",
    "    agent.update(arm, reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01f6fa-d147-4176-bda6-fe2a75e24464",
   "metadata": {},
   "source": [
    "`for t in range(n_trials):` Opakuje se `n_trials` pokusů. Každý simuluje rozhodnutí agenta – výběr páky, získání odměny a aktualizaci znalostí.\n",
    "\n",
    "`arm = agent.select_arm()` Agent zvolí páku podle algoritmu UCB. Agent pro každou páku vygeneruje náhodný vzorek z rozdělení $Beta(úspěchy +1, neúspěchy +1)$ a vybere tu páku, která má nejvyšší vzorek.\n",
    "\n",
    "`reward = rewards[arm, t]` Získá simulovanou odměnu za tahání za zvolenou páku v aktuálním kroku `t`.\n",
    "\n",
    "`agent.update(arm, reward)` Agent aktualizuje počty úspěchů nebo neúspěchů: Pokud `reward` > 0, přičte úspěch. Jinak přičte neúspěch.\n",
    "\n",
    "`total_reward += reward` Přičte získanou odměnu do celkového skóre."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
